from dotenv import load_dotenv
import os
from langchain import OpenAI, LLMChain, PromptTemplate
from langchain.schema import Document
import pdfplumber
from docx import Document as DocxDocument

class LLMQuery:
    def __init__(self):
        load_dotenv()
        openai_api_key = os.getenv("OPENAI_API_KEY")
        self.llm = OpenAI(api_key=openai_api_key)

    def _load_pdf(self, file_path: str) -> str:
        """
        Extract text from a PDF file.
        """
        text = ""
        with pdfplumber.open(file_path) as pdf:
            for page in pdf.pages:
                text += page.extract_text() or ""
        return text

    def _load_word(self, file_path: str) -> str:
        """
        Extract text from a Word file.
        """
        doc = DocxDocument(file_path)
        text = ""
        for para in doc.paragraphs:
            text += para.text + "\n"
        return text

    def _load_documents(self, file_paths: list[str]) -> list[Document]:
        """
        Load documents from a list of file paths (PDF and Word).
        """
        documents = []
        for file_path in file_paths:
            if file_path.lower().endswith('.pdf'):
                content = self._load_pdf(file_path)
            elif file_path.lower().endswith('.docx'):
                content = self._load_word(file_path)
            else:
                raise ValueError(f"Unsupported file format: {file_path}")

            documents.append(Document(content=content))
        return documents

    def generate_query(self, file_paths: list[str], few_shot_prompts: list[str], query: str) -> str:
        """
        Generate a query using the provided documents (PDF and Word files), few-shot prompts, and user query.
        
        :param file_paths: List of file paths to the documents.
        :param few_shot_prompts: List of few-shot examples to guide the model.
        :param query: The user query for which we want to generate a response.
        :return: The response generated by the LLM.
        """
        documents = self._load_documents(file_paths)
        context = "\n".join([doc.content for doc in documents])
        
        prompt_template = "\n".join(few_shot_prompts) + "\n\nContext:\n{context}\n\nQuery:\n{query}\n\nAnswer:"
        prompt = PromptTemplate(
            template=prompt_template,
            input_variables=["context", "query"]
        )

        chain = LLMChain(
            llm=self.llm,
            prompt=prompt
        )

        response = chain.run(context=context, query=query)
        return response
